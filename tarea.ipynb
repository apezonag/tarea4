{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73cbeaf5-c76e-4a33-9ddd-9b30d2bb792c",
   "metadata": {},
   "source": [
    "## EJERCICIOS DE LA SESIÓN - PRÁCTICA 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae397c6-dc2b-4d1b-9318-24d318b8491f",
   "metadata": {},
   "source": [
    "### 1. Crear un repositorio de GitHub para la tarea y clonarlo en local.\n",
    "\n",
    "Para crear un repositorio en GitHub iremos a nuestro perfil, haremos clic sobre \"Repositories\" y posteriormente \"New\". Podremos definir un nombre del repositorio y las opciones que queramos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e6abb-41bb-4701-a283-12c3b16bb48a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Crear dentro del repositorio un entorno de conda—a partir del archivo environment.yml y añadiéndolo al .gitignore—y un notebook de Jupyter.\n",
    "\n",
    "Creamos un repositorio vacío, lo clonamos. Una vez clonado, copiamos el archivo environment.yml, creamos un archivo .gitignore donde incluiremos las siguientes líneas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa599ff5-f589-412e-be8b-59170d808f05",
   "metadata": {},
   "source": [
    "tarea_python y .ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9bdd6-47e7-40dd-9bb2-25de6e0bb606",
   "metadata": {},
   "source": [
    "Ahora crearemos el entorno de conda con las instrucciones de clase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636d4f1-8548-4835-8df6-7a4c9ab7e43d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Enviar todo al repositorio remoto en GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4aa5b4-1189-4c70-9f06-e288242c7dd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4. Cargar un audio estéreo de entre los facilitados y mostrar sus características: frecuencia de muestreo, número de canales y tamaño del archivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0568e-ca1e-463d-9cce-2943bddc0d88",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>cwd = os.getcwd()</b>: Esta línea obtiene el directorio actual de trabajo (cwd) mediante la función getcwd() del módulo os.</li>\n",
    "\n",
    "<li><b>audio_input_path = os.path.join(cwd, os.path.join('audio', '_input'))</b>: Esta línea utiliza la función join() del módulo os para unir el directorio actual de trabajo (cwd) con la carpeta de audio de entrada ('audio'), y luego unir ese resultado con la subcarpeta de entrada ('_input') dentro de la carpeta de audio. El resultado final es la ruta completa al directorio de entrada de audio.</li>\n",
    "\n",
    "<li><b>audio_output_path = os.path.join(cwd, os.path.join('audio', '_output'))</b>: Esta línea hace lo mismo que la anterior, pero en lugar de unir la carpeta de entrada, une la carpeta de salida ('_output').</li>\n",
    "\n",
    "<li><b>print(f'Path to input audio: {audio_input_path}')</b>: Esta línea imprime en la consola la ruta completa al directorio de entrada de audio.</li>\n",
    "\n",
    "<li><b>print(f'Path to output audio: {audio_output_path}\\n')</b>: Esta línea imprime en la consola la ruta completa al directorio de salida de audio, con un salto de línea adicional al final para una mejor legibilidad.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d0f2c-07c8-45df-b72d-deef9f81c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion.\n",
    "from scipy.io import wavfile\n",
    "import IPython\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directorios que usaremos.\n",
    "cwd = os.getcwd()\n",
    "audio_input_path = os.path.join(cwd, os.path.join('audio', '_input'))  \n",
    "audio_output_path = os.path.join(cwd, os.path.join('audio', '_output')) \n",
    "print(f'Path to input audio: {audio_input_path}') \n",
    "print(f'Path to output audio: {audio_output_path}\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae495b-d673-4605-8f8e-15ee469e635e",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>filename = os.path.join(audio_input_path, 'stereo.wav')</b>: Esta línea utiliza la función join() del módulo os para unir la ruta completa al directorio de entrada de audio (audio_input_path) con el nombre del archivo de audio que se desea leer ('stereo.wav'). El resultado final es la ruta completa al archivo de audio.</li>\n",
    "\n",
    "<li><b>sample_rate, audio_data = wavfile.read(filename)</b>: Esta línea utiliza la función read() del módulo wavfile (que se importa con la línea from scipy.io import wavfile) para leer el archivo de audio (filename). La función devuelve la frecuencia de muestreo (sample_rate) y los datos de audio (audio_data) del archivo.</li>\n",
    "\n",
    "<li><b>print(f'Frecuencia de muestreo (sample rate): {sample_rate/1000} kHz')</b>: Esta línea imprime en la consola la frecuencia de muestreo (sample_rate) del archivo de audio, dividida por 1000 para expresarla en kilohercios. La f-string utilizada para imprimir el resultado incluye el texto \"Frecuencia de muestreo (sample rate): \" y el valor de la frecuencia de muestreo con el formato \"x.xxx kHz\".</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9230e8-3fe9-45df-af95-5a5d6c69e05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el archivo de audio.\n",
    "filename = os.path.join(audio_input_path, 'stereo.wav')\n",
    "sample_rate, audio_data = wavfile.read(filename)\n",
    "print(f'Frecuencia de muestreo (sample rate): {sample_rate/1000} kHz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936bd93-e857-46e0-8f7c-3eb05f420afa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Incluir un widget para reproducir el audio estéreo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9562d6a-978a-434e-8eb5-50083b433d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio_data.T, rate=sample_rate) # .T se pasa únicamente si es audio estéreo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0541fd-ac50-40fc-bff4-9f19841bc338",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Convertir el archivo de audio estéreo a mono y mostrar las características anteriormente mencionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8896d3cc-649b-45fa-91f0-5ef26247ab1d",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>new_data_mono = audio_data.mean(axis=1)</b>: Esta línea calcula la media (mean()) de los datos de audio (audio_data) a lo largo del segundo eje (axis=1). Al hacer esto, se promedia los valores de los dos canales (izquierdo y derecho) para convertir los datos estéreo en datos mono. El resultado es un nuevo array new_data_mono que contiene los datos de audio mono.</li>\n",
    "\n",
    "<li><b>print('Nuevos datos de audio (mono):')</b>: Esta línea imprime en la consola un mensaje para indicar que se van a mostrar los nuevos datos de audio en formato mono.</li>\n",
    "\n",
    "<li><b>print(f'- Nuevo tamaño: {new_data_mono.shape}')</b> Esta línea imprime en la consola el tamaño (shape) del nuevo array new_data_mono. El tamaño indica el número de elementos en cada dimensión del array, por lo que en este caso, el tamaño del array es (n,), donde n es el número total de elementos en el array mono.</li>\n",
    "\n",
    "<li><b>print(f'- Canal unico:  {new_data_mono[:5]}...')</b>: Esta línea imprime en la consola los primeros 5 elementos (new_data_mono[:5]) del nuevo array new_data_mono, seguido de puntos suspensivos para indicar que hay más elementos en el array. Al imprimir los primeros elementos, se puede ver que el nuevo array contiene solo un canal de datos de audio en lugar de dos.</li>\n",
    "    \n",
    "<li><b>new_data_mono = new_data_mono.astype(np.int16)</b>: Esta línea convierte los datos de audio en formato mono (new_data_mono) a un tipo de datos entero de 16 bits con signo (np.int16) utilizando el método astype() de los arrays de NumPy. Esto se hace para asegurarse de que los datos de audio tengan la misma resolución y formato que el archivo original, lo que es importante para poder reproducir el archivo de audio correctamente.</li>\n",
    "    \n",
    "<li><b>print(f'- Resolucion:   {type(new_data_mono[0])}\\n')</b>: Esta línea imprime en la consola el tipo de datos (type()) del primer elemento (new_data_mono[0]) del array de datos de audio convertido. En este caso, se espera que el tipo de datos sea np.int16, que es el tipo de datos al que se convirtieron los datos de audio en la línea anterior. El salto de línea adicional (\\n) se utiliza para una mejor legibilidad en la consola.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9e6d4-17cc-4be0-9d68-47c947414ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos a mono mediante la media por canal (simplificacion).\n",
    "new_data_mono = audio_data.mean(axis=1)  # Column-wise.\n",
    "print('Nuevos datos de audio (mono):')\n",
    "print(f'- Nuevo tamaño: {new_data_mono.shape}')\n",
    "print(f'- Canal unico:  {new_data_mono[:5]}...')\n",
    "\n",
    "# Mantenemos la misma resolucion que antes.\n",
    "new_data_mono = new_data_mono.astype(np.int16)\n",
    "print(f'- Resolucion:   {type(new_data_mono[0])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48046f2-ec33-4ad4-bffb-58582cf98334",
   "metadata": {},
   "source": [
    "Guardamos el audio en un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e160472-1458-4072-90ee-f47fe07f3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el archivo mono a un fichero de tipo wav.\n",
    "wavfile.write(\n",
    "    filename=os.path.join(audio_output_path, 'sample1_mono.wav'),\n",
    "    rate=sample_rate,\n",
    "    data=new_data_mono\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca5758-8618-4011-be2e-c68f11077dd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Incluir un nuevo widget para reproducir el audio mono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82f105c-a895-443a-af0e-e56b464b46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(new_data_mono, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa02e64-8f85-4a61-98f5-2c42b8c2c6ca",
   "metadata": {},
   "source": [
    "### 8. Mostrar la gráfica en el dominio del tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033a3086-84cd-4cd5-85e5-128260f2a1da",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\n",
    "<li><b>sample_rate_48, audio_data_48 = wavfile.read(filename=os.path.join(audio_input_path, 'stereo.wav')) </b>: La línea de código lee un archivo de audio de nombre stereo.wav ubicado en el directorio audio/_input y devuelve la frecuencia de muestreo del archivo (sample_rate_48) y los datos de audio (audio_data_48) en formato estéreo con una frecuencia de muestreo de 48 kHz.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7be39e-b92c-4165-9d60-5c31b8718cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los archivos de audio.\n",
    "sample_rate_48, audio_data_48 = wavfile.read(filename=os.path.join(audio_input_path, 'stereo.wav'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41c838-a5b2-4192-8421-0902ec7d439a",
   "metadata": {},
   "source": [
    "La primera línea calcula la longitud de los datos de audio (audio_data) y la asigna a la variable ampl_values_48. La segunda línea imprime el número de muestras en el archivo de audio (ampl_values_48) en la consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5e373-cde0-4763-b6bc-3e7e0580e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ampl_values_48 = len(audio_data)\n",
    "print(f'Número de muestras del archivo (valores de amplitud): {ampl_values_48}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9694e-ff3b-47e4-b1e5-47282e1e80d6",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>t1 = np.arange(0, ampl_values_48/sample_rate_48, 1/sample_rate_48)</b> utiliza la función arange() del módulo numpy para crear un array t1 que representa una secuencia de tiempo discreta en función del número de muestras y la frecuencia de muestreo del archivo de audio. Los argumentos de la función arange() son el inicio de la secuencia (0), el final de la secuencia (la duración total del archivo de audio) y el paso entre valores de tiempo (inverso de la frecuencia de muestreo).</li>\n",
    "\n",
    "<li><b>fig, ax = plt.subplots(1, 1, figsize=(12, 6), sharex=True)</b> crea una nueva figura de tamaño 12x6 pulgadas (figsize=(12, 6)) y un solo conjunto de ejes (ax = plt.subplots(1, 1)). El argumento sharex=True asegura que los subtrazados compartan el mismo eje x.</li>\n",
    "\n",
    "<li><b>end = 50</b> crea una variable end con un valor de 50, que se utiliza más adelante para limitar la longitud del audio que se muestra en el gráfico.</li>\n",
    "\n",
    "<li><b>ax.plot(t1[:end], audio_data_48[:end])</b> traza una línea que representa los datos de audio (audio_data_48) en función del tiempo (t1). El argumento [:end] limita los datos a los primeros 50 valores (o menos, si el archivo de audio es más corto).</li>\n",
    "    \n",
    "<li><b>ax.set_title(f'Audio en el dominio del tiempo muestreado a {sample_rate_48} Hz')</b> establece un título para el gráfico que describe la frecuencia de muestreo del archivo de audio.</li>\n",
    "    \n",
    "<li><b>ax.grid(True)</b> activa la cuadrícula en el gráfico para mejorar la legibilidad.</li>\n",
    "\n",
    "<li><b>plt.tight_layout()</b> ajusta el diseño del gráfico para que no se superpongan los elementos.</li>\n",
    "    \n",
    "<li><b>plt.show()</b> muestra el gráfico en una ventana emergente.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e860f-28cc-499e-b19e-bdd178e53c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valor inicial: 0\n",
    "# Valor final: muestras/frecuencia de muestreo\n",
    "# Incremento: 1/frecuencia de muestreo\n",
    "t1 = np.arange(0, ampl_values_48/sample_rate_48, 1/sample_rate_48)\n",
    "\n",
    "\n",
    "# Mostrar gráfica.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6), sharex=True)\n",
    "end = 50\n",
    "\n",
    "ax.plot(t1[:end], audio_data_48[:end])\n",
    "ax.set_title(f'Audio en el dominio del tiempo muestreado a {sample_rate_48} Hz')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39adcd-4a72-40e4-ae64-e192e88e08e8",
   "metadata": {},
   "source": [
    "#### Dominio en el tiempo mono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f08573-ed2e-4e42-9a00-44b16eaa76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los archivos de audio.\n",
    "sample_rate_48, audio_data_48 = wavfile.read(filename=os.path.join(audio_output_path, 'sample1_mono.wav'))\n",
    "\n",
    "ampl_values_48 = len(new_data_mono)\n",
    "print(f'Número de muestras del archivo (valores de amplitud): {ampl_values_48}')\n",
    "\n",
    "# Valor inicial: 0\n",
    "# Valor final: muestras/frecuencia de muestreo\n",
    "# Incremento: 1/frecuencia de muestreo\n",
    "t1 = np.arange(0, ampl_values_48/sample_rate_48, 1/sample_rate_48)\n",
    "\n",
    "\n",
    "# Mostrar gráfica.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6), sharex=True)\n",
    "end = 50\n",
    "\n",
    "ax.plot(t1[:end], audio_data_48[:end])\n",
    "ax.set_title(f'Audio en el dominio del tiempo muestreado a {sample_rate_48} Hz')\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99f6b2-f6ea-480b-a8f6-3501bab3e88c",
   "metadata": {},
   "source": [
    "### 9. Explicación de los términos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc4f1f-cba8-4576-926d-957fda90089a",
   "metadata": {},
   "source": [
    "\n",
    "<b>Audio estéreo y mono:</b> El audio estéreo utiliza dos canales de audio (izquierdo y derecho) para crear una experiencia de sonido envolvente. En cambio, el audio mono utiliza solo un canal para la reproducción de sonido. En términos técnicos, el audio estéreo contiene más información y requiere más espacio de almacenamiento que el audio mono.\n",
    "\n",
    "<b>Frecuencia de muestreo:</b> La frecuencia de muestreo es el número de veces que se toma una muestra del audio en un segundo. Se mide en Hertz (Hz) y cuanto mayor sea la frecuencia de muestreo, mayor será la calidad del audio grabado.\n",
    "\n",
    "<b>Aliasing:</b> El aliasing es un efecto no deseado que se produce cuando la frecuencia de muestreo no es lo suficientemente alta para capturar las frecuencias más altas presentes en la señal de audio. El resultado es una señal de audio distorsionada o \"sobre-muestreada\".\n",
    "\n",
    "<b>Profundidad de bits:</b> La profundidad de bits se refiere a la cantidad de bits utilizados para representar cada muestra de audio. Cuanto mayor sea la profundidad de bits, mayor será la resolución de la señal de audio y, por lo tanto, mayor será la calidad del sonido.\n",
    "\n",
    "<b>Ancho de banda:</b> El ancho de banda es el rango de frecuencias que se pueden transmitir a través de un medio de comunicación o grabar en un archivo de audio. En el caso de los archivos de audio, el ancho de banda está determinado por la frecuencia de muestreo y la profundidad de bits.\n",
    "\n",
    "<b>Tasa de bits:</b> La tasa de bits es la cantidad de bits que se transmiten o graban por segundo en un archivo de audio. Se calcula multiplicando la frecuencia de muestreo por la profundidad de bits y el número de canales (mono o estéreo). Cuanto mayor sea la tasa de bits, mayor será la calidad del sonido.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710398c9-60db-471a-8d4e-8125a08a710f",
   "metadata": {},
   "source": [
    "### 10. Aplicar la Transformada rápida de Fourier (FFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac14930a-298a-47a0-9e3b-924517b75dd6",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li><b>n = len(audio_data_48)</b> se está utilizando la función len() para obtener la cantidad de muestras que tiene la señal de audio audio_data_48. Esta cantidad de muestras se guarda en la variable n.</li>\n",
    "\n",
    "<li><b>Fs = sample_rate_48</b> se está asignando el valor de la frecuencia de muestreo sample_rate_48 a la variable Fs. La frecuencia de muestreo se utiliza para convertir las muestras de la señal de audio a una forma digital, por lo que es una información importante para trabajar con archivos de audio en el dominio digital.</li>\n",
    "\n",
    "<li><b>ch_Fourier = np.fft.fft(audio_data_48)</b> calcula la transformada de Fourier discreta de la señal de audio audio_data_48 utilizando la función fft del módulo numpy.fft, y guarda los coeficientes de la transformada de Fourier en la variable ch_Fourier. La transformada de Fourier es una herramienta matemática que se utiliza para analizar señales de audio en el dominio de la frecuencia.</li>\n",
    "\n",
    "<li><b>abs_ch_Fourier = np.absolute(ch_Fourier[:n//2])</b> calcula el valor absoluto de los coeficientes de la transformada de Fourier ch_Fourier utilizando la función absolute del módulo numpy. Luego, se guarda el resultado en la variable abs_ch_Fourier.\n",
    "\n",
    "La parte [:n//2] indica que se toman sólo los primeros n//2 coeficientes de la transformada de Fourier, que corresponden a las frecuencias positivas.</li>\n",
    "    \n",
    "<li><b>plt.plot(np.linspace(0, Fs/2, n//2), abs_ch_Fourier)</b> se utiliza para graficar la amplitud del espectro de Fourier abs_ch_Fourier en función de la frecuencia. En este caso, np.linspace(0, Fs/2, n//2) se utiliza para crear un arreglo de valores de frecuencia que va desde 0 Hz hasta la frecuencia de muestreo Fs/2 (ya que la transformada de Fourier de un audio es simétrica, solo se grafica hasta la mitad).</li>\n",
    "    \n",
    "<li><b>plt.ylabel('Amplitud', labelpad=10)</b> se utiliza para establecer una etiqueta en el eje y de la gráfica, que en este caso es \"Amplitud\". Además, labelpad=10 se utiliza para establecer el espaciado de la etiqueta con respecto al eje y.</li>\n",
    "\n",
    "<li><b>plt.xlabel('$f$ (Hz)', labelpad=10)</b> e utiliza para establecer una etiqueta en el eje x de la gráfica, que en este caso es \"$f$ (Hz)\". Además, labelpad=10 se utiliza para establecer el espaciado de la etiqueta con respecto al eje x.</li>\n",
    "    \n",
    "<li><b>plt.show()</b> muestra el gráfico en una ventana emergente.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0b2c4-93af-4af8-ad62-49fafc21d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La longitud del array de datos y el\n",
    "# sample rate (frecuencia de muestreo).\n",
    "n = len(audio_data_48)\n",
    "Fs = sample_rate_48\n",
    "\n",
    "# Working with stereo audio, there are two channels in the audio data.\n",
    "# Let's retrieve each channel seperately:\n",
    "# ch1 = np.array([data[i][0] for i in range(n)]) #channel 1\n",
    "# ch2 = np.array([data[i][1] for i in range(n)]) #channel 2\n",
    "# We can then perform a Fourier analysis on the first\n",
    "# channel to see what the spectrum looks like.\n",
    "\n",
    "# Calculando la Transformada Rapida de Fourier (FFT) en audio mono.\n",
    "ch_Fourier = np.fft.fft(audio_data_48)  # ch1\n",
    "\n",
    "# Solo miramos frecuencia por debajo de Fs/2\n",
    "# (Nyquist-Shannon) --> Spectrum.\n",
    "abs_ch_Fourier = np.absolute(ch_Fourier[:n//2])\n",
    "\n",
    "# Graficamos.\n",
    "plt.plot(np.linspace(0, Fs/2, n//2), abs_ch_Fourier)\n",
    "plt.ylabel('Amplitud', labelpad=10)\n",
    "plt.xlabel('$f$ (Hz)', labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda52ae2-145c-4b71-a768-79d5a97694fe",
   "metadata": {},
   "source": [
    "### 11. Espectograma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69d203-98e6-4c9d-9a07-ba1d1c2fddef",
   "metadata": {},
   "source": [
    "Utilizamos \"epsilon\" como un parámetro de regularización, que se utiliza en técnicas de procesamiento de señales, como la reducción de ruido o la eliminación de artefactos de compresión.\n",
    "\n",
    "En la reducción de ruido, un valor de \"epsilon\" más grande puede eliminar demasiadas componentes de frecuencia de la señal, lo que puede hacer que el sonido resultante parezca menos natural o incluso distorsionado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a7d895-00bf-4b2d-9f33-a78d67e50a5f",
   "metadata": {},
   "source": [
    "#### 11.1 Usando epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba22fca-d34b-40e5-8859-d1c8efcb22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos epsilon: la parte de la energia\n",
    "# del espectro que no conservamos.\n",
    "eps = [1e-5, .02, .041, .063, .086, .101, .123]\n",
    "\n",
    "# Jugamos con los valores de epsilon (CAMBIAD ESTO).\n",
    "eps = eps[6]\n",
    "print(f'Epsilon: {eps}')\n",
    "\n",
    "# Calculamos el valor de corte para esta energia.\n",
    "thr_spec_energy = (1 - eps) * np.sum(abs_ch_Fourier)\n",
    "print(f'Valor de corte para la energia del espectro: {thr_spec_energy}')\n",
    "\n",
    "# Integral de la frecuencia --> energia del espectro.\n",
    "spec_energy = np.cumsum(abs_ch_Fourier)\n",
    "\n",
    "# Mascara (array booleano) que compara el valor\n",
    "# de corte con la energia del espectro.\n",
    "frequencies_to_remove = thr_spec_energy < spec_energy  \n",
    "print(f'Mascara: {frequencies_to_remove}')\n",
    "\n",
    "# La frecuencia f0 por la que cortamos el espectro.\n",
    "f0 = (len(frequencies_to_remove) - np.sum(frequencies_to_remove)) * (Fs/2) / (n//2)\n",
    "print(f'Frecuencia de corte f0 (Hz): {int(f0)}')\n",
    "\n",
    "# Graficamos.\n",
    "plt.axvline(f0, color='r')\n",
    "plt.plot(np.linspace(0, Fs/2, n//2), abs_ch_Fourier)\n",
    "plt.ylabel('Amplitud')\n",
    "plt.xlabel('$f$ (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68532f65-e30f-40ca-bd98-1f47906598fb",
   "metadata": {},
   "source": [
    "### 12. Downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c1230b-59ca-46e3-9385-bb4f32d76da9",
   "metadata": {},
   "source": [
    "El downsampling es una técnica de procesamiento de señales de audio que consiste en reducir la frecuencia de muestreo de una señal de audio. La frecuencia de muestreo se refiere a la cantidad de muestras de audio que se toman por segundo para representar la señal.\n",
    "\n",
    "El downsampling se utiliza comúnmente para reducir el tamaño de un archivo de audio, disminuir la carga computacional en el procesamiento de señales y reducir el ancho de banda requerido para transmitir la señal a través de una red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936831c-026a-4e23-9774-2ffa204366e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los nombres de los audios comprimidos.\n",
    "wav_compressed_file = \"prueba.wav\"\n",
    "\n",
    "# Calculamos el factor D de downsampling.\n",
    "D = int(Fs / f0)\n",
    "print(f'Factor de downsampling: {D}')\n",
    "\n",
    "# Obtenemos los nuevos datos (slicing with stride).\n",
    "new_data = audio_data_48[::D]\n",
    "\n",
    "# Escribimos los datos a un archivo de tipo wav.\n",
    "wavfile.write(\n",
    "    filename=os.path.join(audio_output_path, wav_compressed_file),\n",
    "    rate=int(Fs/D),\n",
    "    data=new_data\n",
    ")\n",
    "\n",
    "# Cargamos el nuevo archivo.\n",
    "new_sample_rate, new_audio_data = wavfile.read(filename=os.path.join(audio_output_path, wav_compressed_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34cb145-248a-4fdd-ab21-cc1047760a57",
   "metadata": {},
   "source": [
    "### 13. Espectograma de ambas ondas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3a36c-9110-4289-84dd-8edcb8b29309",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "Pxx, freqs, bins, im = ax[0].specgram(audio_data_48, NFFT=1024, Fs=sample_rate_48, noverlap=512)\n",
    "ax[0].set_title('Espectograma del audio original')\n",
    "ax[0].set_ylabel('Frecuencia (Hz)')\n",
    "ax[0].grid(True)\n",
    "\n",
    "Pxx, freqs, bins, im = ax[1].specgram(new_audio_data, NFFT=1024, Fs=new_sample_rate, noverlap=512)\n",
    "ax[1].set_title('Espectrograma del audio reducido/comprimido')\n",
    "ax[1].set_xlabel('Tiempo (s)')\n",
    "ax[1].set_ylabel('Frecuencia (Hz)')\n",
    "ax[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ea5b0-2cd6-426a-8e76-0fb7aa5bf74f",
   "metadata": {},
   "source": [
    "El espectrograma de un audio es una representación visual de la energía de las diferentes frecuencias que componen el sonido a lo largo del tiempo.\n",
    "\n",
    "Cuando un audio es reducido o comprimido, se eliminan algunos de los datos de la señal original para reducir el tamaño del archivo de audio. En este proceso, se pueden perder detalles de la señal original, lo que puede afectar la calidad del sonido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313cd85-bde1-480d-8ddf-321c3f507ade",
   "metadata": {},
   "source": [
    "#### 13.1 Mostramos el tamaño de ámbos archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e71afb-151b-44b0-bf0a-d3f4f1e70ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -sh audio/_output/sample1_mono.wav\n",
    "!ls -sh audio/_output/prueba.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59684b3-23ac-49b3-99be-e866a7019771",
   "metadata": {},
   "source": [
    "#### 13.2 Audios de los dos sonidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bfff1-f76e-4f3c-b253-e3045b7af740",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Audio sin reducción\")\n",
    "IPython.display.Audio(audio_data_48, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cd325-3048-4d60-8255-c3ec945ea25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Audio con reducción\")\n",
    "IPython.display.Audio(new_audio_data, rate=new_sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tarea",
   "language": "python",
   "name": "tarea"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
